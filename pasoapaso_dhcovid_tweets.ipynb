{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling paso a paso: los temas de la pandemia\n",
    "\n",
    "Esta notebook muestra las etapas de una experiencia de modelización de tópicos con un corpus de tweets sobre la pandemia de coronavirus. Aquí explicamos cómo:\n",
    "\n",
    "- preprocesar los datos\n",
    "- entrenar modelos con LDA\n",
    "- generar visualizaciones de los resultados\n",
    "\n",
    "Autora: Nidia Hernández, CAICYT-CONICET, nidiahernandez@conicet.gov.ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "\n",
    "Primero, nos aseguramos de instalar las librerías necesarias y otros requerimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#! pip install requirements.txt\n",
    "! python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías y las funciones que vamos a usar para el procesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "from os import makedirs\n",
    "import re\n",
    "\n",
    "from detectar_topicos import * # Importa las funciones del script detectar_topicos.py\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos https://covid.dh.miami.edu/get/ para obtener una colección de tweets por fecha. Los tweets son descargados en formato txt, un tweet por línea. \n",
    "\n",
    "Aquí vamos a trabajar con los tweets sobre el covid19 del 6 de noviembre de 2020 en Argentina, el día en finalizó el aislamiento social preventivo y obligatorio en la Ciudad de Buenos Aires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets totales:  2177\n",
      "Tweets sin duplicados: 2142\n"
     ]
    }
   ],
   "source": [
    "corpus_path = 'dhcovid_2020-11-06_es_ar.txt'\n",
    "corpus_label = corpus_path.replace('.txt', '')\n",
    "\n",
    "with open(corpus_path, 'r') as fi:\n",
    "    tweets = fi.read()\n",
    "    print('Tweets totales: ', len(tweets.split('\\n')))\n",
    "    tweets = set(tweets.split('\\n') ) # elimina duplicados\n",
    "    tweets = list(list(tweets))\n",
    "\n",
    "print(f'Tweets sin duplicados: {len(tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los diez primeros tweets de esta colección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la defensa covid19 76 casos 2 muertos URL',\n",
       " 'en vivo reporte desde el ministerio de salud nuevo URL',\n",
       " 'casado arranca su segunda temporada en italia en medio de los rebrotes de covid19 URL',\n",
       " 'spoiler si trump pierde las elecciones el covid19 desaparece si trump gana la 2da ola va a ser aun peor que la primera cada dia mas convencido de que es un virus intencional',\n",
       " 'por un lado tengo a mi viejo que le sube un poco la fiebre y ya llora por su muerte y por el otro tengo al obsesivo por la limpieza llorando por q siente el covid19 en sus manos y recien estamos en el primer dia de aislamiento q hice en mi otra vida para merecer 2 trolos llorones',\n",
       " 'otra noche otro servicio otro accidente otra vez cambiandome con la proteccion para el covid19 otra vez acostandome a las 4 am',\n",
       " 'apocrifo 127 pizza con covid19 mientras estaba cenando en el local de @user del cerro de las rosas llego la policia para avisar que un empleado habia dado positivo de covid19 #yomequedoencasa #coronavirus URL URL',\n",
       " 'necesito pasar un fin de semana con mi amiga xq mi salud mental me lo pide covid19 hdp',\n",
       " 'al fin aprendio tu cuerpo tu decision #abortolegal2020 pd no te vacunes mejor asi el covid19 hace lo suyo y un dinosaurio URL',\n",
       " 'covid en la pampa 111 nuevos contagios solo 1 en realico URL',\n",
       " 'estoy mas cansando de convivir con mi familia que del covid',\n",
       " '#viernes de #fbf y recordamos el segundo #webinar de la serie de #biotecnologia y #salud sobre terapias anti sars cov 2 experiencias en latinoamerica mira el video en URL #biotech #health #covid #covid19 #friday #flashbackfriday',\n",
       " 'es fundamental conocer el protocolo puesto en marcha en esta residencia de puente genil para saber que ha fallado y evitar que se repita llevaremos al parlamento esta demanda de las familias que tambien piden transparencia sobre la salud de sus mayores URL',\n",
       " 'sos mas falso q los caso de covid19 de sj',\n",
       " 'mis amigas me dicen que tengo cara de muerta que estoy super ojerosa que si no tengo covid19 porque hasta me cambio la voz que estoy consumida que estoy re demacrada pa ke quiero enemigosno']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchos tokens irrelevantes para nuestro objetivo de detección de tópicos: 'URL', '@user', números, palabras vacías de contenido informativo, etc.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "Es habitual mejorar la calidad de los datos de entrada del topic modeling realizando diversos tipos de preprocesamiento. Los más rápidos y sencillos son eliminar tokens poco pertinentes. Otros pueden ser computacionalmente más costosos pero mejoran notablemente la legibilidad de los resultados. En esta experiencia, realizaremos un pretratamiento standard: filtrar tokens, generar bigramas y marcar Named Entities.\n",
    "\n",
    "Creamos una etiqueta para identificar los modelos de acuerdo al pretratamiento que vamos a realizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = '2gram_ner_LDA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, eliminamos las transliteraciones de emojis usando la función `remove_emojis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_noemojis = [remove_emojis(tweet).strip() for tweet in tweets]\n",
    "tweets_noemojis = [tweet for tweet in tweets_noemojis if tweet]\n",
    "tweets_noemojis = [tweet.split() for tweet in tweets_noemojis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, eliminamos las palabras gramaticales ('la', 'que', 'de', etc) ya que son muy frecuentes pero no aportan información temática significativa. En el ámbito del _text processing_ estas palabras son conocidas como _stopwords_. Cargamos una lista genérica de _stopwords_ del español y le añadimos tokens particulares de nuestro corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dhcovid_2020-11-06_es_ar-2gram_ner_LDA] Filtrando stopwords\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words_extra = ['@user', '#covid19', '#covid','#coronavirus','URL','xq','pq', 'q', 'd', 'x', 'e', 'k', 'l', 're','ja', 'jaja'\n",
    "                    'si', 'mas','da','dia', 'hoy', 'año', 'aca', 'ahi', 'aqui', 'vez', 'tras', 'traves', 'bueno']\n",
    "stop_words = stop_words+stop_words_extra\n",
    "\n",
    "print(f\"[{corpus_label}-{model_label}] Filtrando stopwords\")\n",
    "tweets_filtrados = [[token for token in texto if token not in stop_words] for texto in tweets_noemojis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa que conservar expresiones como \"nuevo caso\" o \"vacuna rusa\" porque ayudan notablemente a la lectura de resultados frente a las mismas palabras por separado. Esto lo logramos generando los bigramas de los tweets y conservando los que aparecen al menos 15 veces en la colección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dhcovid_2020-11-06_es_ar-2gram_ner_LDA] Generando bigramas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2142/2142 [00:00<00:00, 59957.02it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{corpus_label}-{model_label}] Generando bigramas\")\n",
    "bigram = Phrases(tweets, min_count=15)\n",
    "\n",
    "tweets_bigrams = tweets_filtrados.copy()\n",
    "for idx in tqdm(range(len(tweets_filtrados))):\n",
    "    for token in bigram[tweets_filtrados[idx]]:\n",
    "        if '_' in token:\n",
    "            tweets_bigrams[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos los números después de generar los bigramas para conservar expresiones como '24_horas'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_bigrams_filt = [[token for token in texto if not token.isnumeric()] for texto in tweets_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro procesamiento que permite lograr resultados más claros para la lectura humana es la identificación de Named Entities. Esta técnica nos permite detectar expresiones como 'hospital ramos mejia' para marcarlas así 'hospital_ramos_mejia'.\n",
    "\n",
    "Previamente, usamos Spacy para detectar automáticamente las Named Entities de esta colección y las volcamos en una lista que revisamos manualmente para eliminar falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy_nlp = spacy.load('es_core_news_lg')\n",
    "# tweets_spacy = [spacy_nlp(' '.join(tweet), disable=[\"tagger\", \"parser\"]) for tweet in tqdm(tweets_bigrams_filt)]\n",
    "\n",
    "# with open(f'{corpus_label}_NE.lst', 'w') as fi:\n",
    "#     for tweet in tweets_spacy:\n",
    "#         for entity in tweet.ents:\n",
    "#             entity_words = str(entity).split()\n",
    "#             if len(entity_words) > 1:\n",
    "#                 fi.write(f'{\" \".join(entity_words)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 140/2142 [00:00<00:01, 1392.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dhcovid_2020-11-06_es_ar-2gram_ner_LDA] Identificando Named Entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2142/2142 [00:01<00:00, 1382.45it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{corpus_label}-{model_label}] Identificando Named Entities\")\n",
    "with open(f'dhcovid_es_ar_NE.lst', 'r') as fi: # lista de NE revisada manualmente\n",
    "    entidades_curadas = fi.read().split('\\n')\n",
    "\n",
    "tweets_ner = []\n",
    "for texto in tqdm(tweets_bigrams_filt):\n",
    "    texto = ' '.join(texto)\n",
    "    for entity in entidades_curadas:\n",
    "        entity_merged = '_'.join(entity.split())\n",
    "        texto = texto.replace(entity, entity_merged)\n",
    "    tweets_ner.append(texto.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas estas manipulaciones del corpus pueden ser costosas, por eso conviene guardar una copia del resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dhcovid_2020-11-06_es_ar] Guardando copia de tweets procesados\n"
     ]
    }
   ],
   "source": [
    "processed_tweets_path = corpus_path.replace(\".txt\", \".processed-tweets.json\")\n",
    "\n",
    "print(f\"[{corpus_label}] Guardando copia de tweets procesados\")\n",
    "dump_processed_tweets_as_json(tweets_ner, processed_tweets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Una vez que los datos de entrada fueron adaptados, podemos proceder al aprendizaje no supervisado de tópicos. Como los tópicos emergen automáticamente de los datos, no podemos saber de antemano cuántos serán. El parámetro `topic_numbers_to_try` permite configurar el rango de tópicos a entrenar.\n",
    "\n",
    "En este paso, también podemos refinar el corpus de entranda. El parámetro `filter_extremes` permite excluir palabras de frecuencia muy baja o demasiado alta (ver la función `make_dictionary_and_matrix` en `detectar_topicos.py`).\n",
    "\n",
    "⚠️ El aprendizaje puede llevar muchas horas si el corpus es grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dhcovid_2020-11-06_es_ar-2gram_ner_LDA] Unsupervised learning\n",
      "Training LDA model with 3 topics\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d8b8aedd2e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# False: carga modelos previamente entrenados si existen. True: los reescribe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfilter_extremes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/Topic-modeling-gestion-datos/detectar_topicos.py\u001b[0m in \u001b[0;36mtrain_several_LDA_models\u001b[0;34m(documentos, topic_numbers_to_try, corpus_label, model_label, models_dir, overwrite, filter_extremes)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dictionary_and_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_extremes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mtrain_LDA_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Already trained for {ntopics} topics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Topic-modeling-gestion-datos/detectar_topicos.py\u001b[0m in \u001b[0;36mtrain_LDA_model\u001b[0;34m(ntopics, dictionary, doc_term_matrix, output_path, seed)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    978\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                     )\n\u001b[0;32m--> 980\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0;31m# Contribution of document d to the expected sufficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# statistics for the M step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0msstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"[{corpus_label}-{model_label}] Unsupervised learning\")\n",
    "models_dir='./dhcovid_tweets_models'\n",
    "\n",
    "models = train_several_LDA_models(\n",
    "    documentos=tweets_ner,\n",
    "    topic_numbers_to_try=range(3, 51),\n",
    "    corpus_label=corpus_label,\n",
    "    model_label=model_label,\n",
    "    models_dir=models_dir,\n",
    "    overwrite=True, # False: carga modelos previamente entrenados si existen. True: los reescribe.\n",
    "    filter_extremes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo saber cuál es el modelo con el número óptimo de tópicos?\n",
    "\n",
    "## Evaluación automática\n",
    "\n",
    "En el paso anterior, entrenamos modelos para un amplio rango de tópicos. ¿Cómo saber cuál es el que mejor representa las temáticas que se tratan en nuestro corpus de tweets de Argentina del 6 de noviembre de 2020 sobre el covid19?\n",
    "\n",
    "Una manera de encontrar _automáticamente_ el modelo con el mejor número de tópicos es usar un score de coherencia. Existen varias medidas que permiten evaluar la coherencia de los modelos generados. En esta notebook vamos a usar _cv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = calculate_topic_coherence(models, tweets_ner, measures=[\"c_v\"], filter_extremes=True)\n",
    "scores.to_csv(f'{models_dir}/{corpus_label}-{model_label}-coherence.csv')\n",
    "plot_cv(scores, corpus_label, model_label, models_dir)\n",
    "\n",
    "ntopics_with_top_cv_score = scores.set_index(\"ntopics\").c_v.idxmax()\n",
    "cv_score = round(scores.c_v.max(), 2)\n",
    "\n",
    "print(f\"El modelo más coherente tiene {ntopics_with_top_cv_score} tópicos y recibió un score de c_v de {cv_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El score automático nos permite descartar rápidamente varios modelos, pero es necesario encontrar un compromiso entre el valor del score y un número de tópicos razonable para la evaluación cualitativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de resultados\n",
    "\n",
    "Cada modelo tiene una cantidad de tópicos posibles. A su vez, cada tópico está integrado por las palabras más probables para ese tópico y la probabilidad asociada. Para visualizar toda esta información, construimos una tabla con los tópicos, las palabras y las probabilidades para el modelo que recibió el mejor scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[ntopics_with_top_cv_score]\n",
    "tabla = make_table_all_topics(best_model, model_label, corpus_label)\n",
    "tabla.to_csv(f'{models_dir}/{corpus_label}-{model_label}-topics.csv')\n",
    "tabla.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para cada tópico, visualizamos las palabras que lo componen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_total = tabla.topic_id.unique()\n",
    "for topic_id in topic_total:\n",
    "    topic = tabla.query(\"topic_id == @topic_id\")\n",
    "    print(f'Tópico {topic_id}: ', \" | \".join(topic.word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos generar una visualización interactiva con más información donde podemos observar fácilmente cantidad de tópicos, distancia, peso y palabras de cada tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_LDA_topics(best_model, tweets_ner, output_path=f'{models_dir}/{corpus_label}-{model_label}.html', filter_extremes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling paso a paso: los temas de la pandemia\n",
    "\n",
    "Esta notebook muestra las etapas de una experiencia de modelización de tópicos con un corpus de tweets sobre la pandemia de coronavirus. Aquí explicamos cómo:\n",
    "\n",
    "- preprocesar los datos\n",
    "- entrenar modelos con LDA\n",
    "- generar visualizaciones de los resultados\n",
    "\n",
    "Autora: Nidia Hernández, CAICYT-CONICET, nidiahernandez@conicet.gov.ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "\n",
    "Primero, nos aseguramos de instalar las librerías necesarias y otros requerimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install requirements.txt\n",
    "! python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías y las funciones que vamos a usar para el procesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "from os import makedirs\n",
    "import re\n",
    "\n",
    "from detectar_topicos import * # Importa las funciones del script detectar_topicos.py\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos https://covid.dh.miami.edu/get/ para obtener una colección de tweets por fecha. Los tweets son descargados en formato txt, un tweet por línea. \n",
    "\n",
    "Aquí vamos a trabajar con los tweets sobre el covid19 del 6 de noviembre de 2020 en Argentina, el día en finalizó el aislamiento social preventivo y obligatorio en la Ciudad de Buenos Aires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'dhcovid_2020-11-06_es_ar.txt'\n",
    "corpus_label = corpus_path.replace('.txt', '')\n",
    "\n",
    "with open(corpus_path, 'r') as fi:\n",
    "    tweets = fi.read()\n",
    "    tweets = set(tweets.split('\\n') ) # elimina duplicados\n",
    "    tweets = list(list(tweets))\n",
    "\n",
    "print(f'Hay {len(tweets)} tweets en la colección')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los diez primeros tweets de esta colección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchos tokens irrelevantes para nuestro objetivo de detección de tópicos: 'URL', '@user', números, palabras vacías de contenido informativo, etc.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "Es habitual mejorar la calidad de los datos de entrada del topic modeling realizando diversos tipos de preprocesamiento. Los más rápidos y sencillos son eliminar tokens poco pertinentes. Otros pueden ser computacionalmente más costosos pero mejoran notablemente la legibilidad de los resultados. En esta experiencia, realizaremos un pretratamiento standard: filtrar tokens, generar bigramas y marcar Named Entities.\n",
    "\n",
    "Creamos una etiqueta para identificar los modelos de acuerdo al pretratamiento que vamos a realizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = '2gram_ner_LDA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, eliminamos las transliteraciones de emojis usando la función `remove_emojis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [remove_emojis(tweet).strip() for tweet in tweets]\n",
    "tweets = [tweet for tweet in tweets if tweet]\n",
    "tweets = [tweet.split() for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, eliminamos las palabras gramaticales ('la', 'que', 'de', etc) ya que son muy frecuentes pero no aportan información temática significativa. En el ámbito del _text processing_ estas palabras son conocidas como _stopwords_. Cargamos una lista genérica de _stopwords_ del español y le añadimos tokens particulares de nuestro corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words_extra = ['@user', '#covid19', '#covid','#coronavirus','URL','xq','pq', 'q', 'd', 'x', 'e', 'k', 'l', 're','ja', 'jaja'\n",
    "                    'si', 'mas','da','dia', 'hoy', 'año', 'aca', 'ahi', 'aqui', 'vez', 'tras', 'traves', 'bueno']\n",
    "stop_words = stop_words+stop_words_extra\n",
    "\n",
    "print(f\"[{corpus_label}-{model_label}] Filtrando stopwords\")\n",
    "tweets_filtrados = [[token for token in texto if token not in stop_words] for texto in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa que conservar expresiones como \"nuevo caso\" o \"vacuna rusa\" porque ayudan notablemente a la lectura de resultados frente a las mismas palabras por separado. Esto lo logramos generando los bigramas de los tweets y conservando los que aparecen al menos 15 veces en la colección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[{corpus_label}-{model_label}] Generando bigramas\")\n",
    "bigram = Phrases(tweets, min_count=15)\n",
    "\n",
    "tweets_bigrams = tweets_filtrados.copy()\n",
    "for idx in tqdm(range(len(tweets_filtrados))):\n",
    "    for token in bigram[tweets_filtrados[idx]]:\n",
    "        if '_' in token:\n",
    "            tweets_bigrams[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos los números después de generar los bigramas para conservar expresiones como '24_horas'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_bigrams_filt = [[token for token in texto if not token.isnumeric()] for texto in tweets_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro procesamiento que permite lograr resultados más claros para la lectura humana es la identificación de Named Entities. Esta técnica nos permite detectar expresiones como 'hospital ramos mejia' para marcarlas así 'hospital_ramos_mejia'.\n",
    "\n",
    "Previamente, usamos Spacy para detectar automáticamente las Named Entities de esta colección y las volcamos en una lista que revisamos manualmente para eliminar falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy_nlp = spacy.load('es_core_news_lg')\n",
    "# tweets_spacy = [spacy_nlp(' '.join(tweet), disable=[\"tagger\", \"parser\"]) for tweet in tqdm(tweets_bigrams_filt)]\n",
    "\n",
    "# with open(f'{corpus_label}_NE.lst', 'w') as fi:\n",
    "#     for tweet in tweets_spacy:\n",
    "#         for entity in tweet.ents:\n",
    "#             entity_words = str(entity).split()\n",
    "#             if len(entity_words) > 1:\n",
    "#                 fi.write(f'{\" \".join(entity_words)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[{corpus_label}-{model_label}] Identificando Named Entities\")\n",
    "with open(f'dhcovid_es_ar_NE.lst', 'r') as fi: # lista de NE revisada manualmente\n",
    "    entidades_curadas = fi.read().split('\\n')\n",
    "\n",
    "tweets_ner = []\n",
    "for texto in tqdm(tweets_bigrams_filt):\n",
    "    texto = ' '.join(texto)\n",
    "    for entity in entidades_curadas:\n",
    "        entity_merged = '_'.join(entity.split())\n",
    "        texto = texto.replace(entity, entity_merged)\n",
    "    tweets_ner.append(texto.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas estas manipulaciones del corpus pueden ser costosas, por eso conviene guardar una copia del resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets_path = corpus_path.replace(\".txt\", \".processed-tweets.json\")\n",
    "\n",
    "print(f\"[{corpus_label}] Guardando copia de tweets procesados\")\n",
    "dump_processed_tweets_as_json(tweets_ner, processed_tweets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Una vez que los datos de entrada fueron adaptados, podemos proceder al aprendizaje no supervisado de tópicos. Como los tópicos emergen automáticamente de los datos, no podemos saber de antemano cuántos serán. El parámetro `topic_numbers_to_try` permite configurar el rango de tópicos a entrenar.\n",
    "\n",
    "En este paso, también podemos refinar el corpus de entranda. El parámetro `filter_extremes` permite excluir palabras de frecuencia muy baja o demasiado alta (ver la función `make_dictionary_and_matrix` en `detectar_topicos.py`).\n",
    "\n",
    "⚠️ El aprendizaje puede llevar muchas horas si el corpus es grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[{corpus_label}-{model_label}] Unsupervised learning\")\n",
    "models_dir='./dhcovid_tweets_models'\n",
    "\n",
    "models = train_several_LDA_models(\n",
    "    documentos=tweets_ner,\n",
    "    topic_numbers_to_try=range(3, 51),\n",
    "    corpus_label=corpus_label,\n",
    "    model_label=model_label,\n",
    "    models_dir=models_dir,\n",
    "    overwrite=False, # False: carga modelos previamente entrenados si existen. True: los reescribe.\n",
    "    filter_extremes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo saber cuál es el modelo con el número óptimo de tópicos?\n",
    "\n",
    "## Evaluación automática\n",
    "\n",
    "En el paso anterior, entrenamos modelos para un amplio rango de tópicos. ¿Cómo saber cuál es el que mejor representa las temáticas que se tratan en nuestro corpus de tweets de Argentina de la última semana de octubre 2020 sobre el covid19?\n",
    "\n",
    "Una manera de encontrar _automáticamente_ el modelo con el mejor número de tópicos es usar un score de coherencia. Existen varias medidas que permiten evaluar la coherencia de los modelos generados. En esta notebook vamos a usar _cv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = calculate_topic_coherence(models, tweets_ner, measures=[\"c_v\"], filter_extremes=True)\n",
    "scores.to_csv(f'{models_dir}/{corpus_label}-{model_label}-coherence.csv')\n",
    "plot_cv(scores, corpus_label, model_label, models_dir)\n",
    "\n",
    "ntopics_with_top_cv_score = scores.set_index(\"ntopics\").c_v.idxmax()\n",
    "cv_score = round(scores.c_v.max(), 2)\n",
    "\n",
    "print(f\"El modelo más coherente tiene {ntopics_with_top_cv_score} tópicos y recibió un score de c_v de {cv_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El score automático nos permite descartar rápidamente varios modelos, pero es necesario encontrar un compromiso entre el valor del score y un número de tópicos razonable para la evaluación cualitativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de resultados\n",
    "\n",
    "Cada modelo tiene una cantidad de tópicos posibles. A su vez, cada tópico está integrado por las palabras más probables para ese tópico y la probabilidad asociada. Para visualizar toda esta información, construimos una tabla con los tópicos, las palabras y las probabilidades para el modelo que recibió el mejor scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[ntopics_with_top_cv_score]\n",
    "tabla = make_table_all_topics(best_model, model_label, corpus_label)\n",
    "tabla.to_csv(f'{models_dir}/{corpus_label}-{model_label}-topics.csv')\n",
    "tabla.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para cada tópico, visualizamos las palabras que lo componen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_total = tabla.topic_id.unique()\n",
    "for topic_id in topic_total:\n",
    "    topic = tabla.query(\"topic_id == @topic_id\")\n",
    "    print(f'Tópico {topic_id}: ', \" | \".join(topic.word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos generar una visualización interactiva con más información donde podemos observar fácilmente cantidad de tópicos, distancia, peso y palabras de cada tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_LDA_topics(best_model, tweets_ner, output_path=f'{models_dir}/{corpus_label}-{model_label}.html', filter_extremes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
